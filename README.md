# LLM Emotion Audit Framework

This repository provides a unified framework for **auditing, validating, and
learning from emotions in LLM-generated speech and text**, with a particular
focus on **synthetic data**, **label noise**, and **affective consistency**.

The project is organized around two complementary components:

- **MADE** — a large-scale synthetic monologue dataset with dataset-level
  validation against human-annotated benchmarks.
- **PLEA** — a pseudo-label–aware utterance-level emotion classification
  framework designed to learn robust models under noisy supervision.

Together, these components support systematic analysis of **emotion realism,
label reliability, and learning robustness** in modern LLM-based pipelines.

---

## Repository Structure

```
.
├── MADE/        # Synthetic dataset construction and dataset-level validation
│   └── README.md
├── PLEA/        # Pseudo-label aware utterance-level emotion classification
│   └── README.md
└── README.md    # (this file)
```

---

## MADE: Monologue-based Artificial Discourse for Emotion Analysis

**MADE** focuses on the **dataset side** of the problem.

It introduces a large-scale synthetic monologue dataset generated using
multiple large language models and evaluates its realism through
**dataset-level comparisons** with established human-annotated benchmarks
such as MELD, IEMOCAP, DailyDialog, and GoEmotions.

Key characteristics:
- Single-speaker monologues with sentence-level emotion labels
- Unified 7-class emotion taxonomy
- Structural and statistical quality control
- Dataset-level validation using:
  - emotion distribution similarity
  - semantic embedding alignment
  - affective transition consistency

See [`MADE/README.md`](./MADE/README.md) for full details.

---

## PLEA: Pseudo-Label Aware Emotion Classification

**PLEA** focuses on the **learning side** of the problem.

It provides an utterance-level emotion classification framework designed to
operate under **noisy or synthetic supervision**, such as labels generated by
LLMs. Instead of trusting synthetic labels directly, PLEA uses
**ensemble-based pseudo-labeling** and **confidence-aware training strategies**
to improve robustness.

Key characteristics:
- Context-free, utterance-level emotion classification
- Ensemble teacher generation from external human-annotated datasets
- Pseudo-label–aware training with confidence and agreement weighting
- Direct comparison between LLM-only and ensemble-supervised learning

See [`PLEA/README.md`](./PLEA/README.md) for full details.

---

## Conceptual Workflow

```
LLMs ──▶ MADE (synthetic dataset)
          │
          ├─ Dataset-level validation (distributions, semantics, transitions)
          │
          ▼
        PLEA (utterance-level learning)
          │
          ├─ Ensemble pseudo-label generation
          └─ Noise-robust emotion classification
```

---

## Scope and Design Philosophy

- **Separation of concerns**  
  MADE evaluates *data realism*; PLEA evaluates *learning robustness*.

- **No reliance on ground-truth for synthetic data**  
  Validation and supervision are performed via distributional analysis and
  ensemble agreement.

- **Utterance-level focus**  
  All models operate without contextual information to isolate label quality
  and supervision effects.

---

## Intended Use

This repository is intended for researchers working on:
- emotion analysis in synthetic or LLM-generated data
- label noise and weak supervision
- affective computing and speech emotion recognition
- dataset auditing and validation

---

## Citation

If you use this framework or its components, please cite the project repository:

```bibtex
@misc{ozcan_llm_emotion_audit_2026,
  author       = {Ozcan, Ahmet Remzi},
  title        = {LLM Emotion Audit Framework},
  year         = {2026},
  howpublished = {GitHub repository},
  url          = {https://github.com/arozcan/llm-emotion-audit},
  note         = {Accessed: 2026-01-23}
}
```
---

## License

This project is released for **research purposes only**.
See individual subdirectories for dataset-specific licensing constraints.
